apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-cluster-values
  namespace: ceph
data:
  values.yaml: |
    operatorNamespace: rook-ceph
    clusterName: ceph
    kubeVersion:
    configOverride: |
      [global]
      mon_allow_pool_delete = true
      mon_allow_pool_size_one = true
      osd_pool_default_size = 1
      osd_pool_default_min_size = 1
      mon_warn_on_pool_no_redundancy = false
      auth_allow_insecure_global_id_reclaim = false
    toolbox:
      enabled: true
      tolerations: []
      affinity: {}
      resources:
        limits:
          cpu: "100m"
          memory: "64Mi"
        requests:
          cpu: "100m"
          memory: "64Mi"
      priorityClassName:
    pspEnable: false
    cephClusterSpec:
      cephVersion:
        image: quay.io/ceph/ceph:v19.2.2
        allowUnsupported: false
      dataDirHostPath: /var/lib/rook
      skipUpgradeChecks: false
      continueUpgradeAfterChecksEvenIfNotHealthy: false
      waitTimeoutForHealthyOSDInMinutes: 10
      mon:
        count: 3
        allowMultiplePerNode: false
      mgr:
        count: 3
        allowMultiplePerNode: false
        modules:
          - name: pg_autoscaler
            enabled: true
          - name: dashboard
            enabled: false
          - name: nfs
            enabled: false
      dashboard:
        enabled: true
        ssl: true
      network:
        connections:
          encryption:
            enabled: false
          compression:
            enabled: false
          requireMsgr2: false
        provider: host
      crashCollector:
        disable: true
      logCollector:
        enabled: true
        periodicity: daily
        maxLogSize: 500M
      cleanupPolicy:
        confirmation: ""
        sanitizeDisks:
          method: quick
          dataSource: zero
          iteration: 1
        allowUninstallWithVolumes: false
      removeOSDsIfOutAndSafeToRemove: false
      priorityClassNames:
        mon: system-node-critical
        osd: system-node-critical
        mgr: system-cluster-critical
      storage:
        useAllNodes: true
        useAllDevices: false
        devices:
          - name: "/dev/sdb"
            config:
              databaseSizeMB: "5120"
              walSizeMB: "2048"
      disruptionManagement:
        managePodBudgets: true
        osdMaintenanceTimeout: 30
        pgHealthCheckTimeout: 0
      healthCheck:
        daemonHealth:
          mon:
            disabled: false
            interval: 45s
          osd:
            disabled: false
            interval: 60s
          status:
            disabled: false
            interval: 60s
        livenessProbe:
          mon:
            disabled: false
          mgr:
            disabled: false
          osd:
            disabled: false
    ingress:
      dashboard: {}
    cephBlockPools:
      - name: rbd
        namespace: ceph
        spec:
          failureDomain: host
          replicated:
            size: 1
        storageClass:
          enabled: true
          name: general
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: "Immediate"
          mountOptions: []
          allowedTopologies: []
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/fstype: ext4
    cephFileSystems:
      - name: cephfs
        namespace: ceph
        spec:
          metadataPool:
            replicated:
              size: 1
          dataPools:
            - failureDomain: host
              replicated:
                size: 1
              name: data
          metadataServer:
            activeCount: 1
            activeStandby: false
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          isDefault: false
          name: ceph-filesystem
          pool: data0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: "Immediate"
          mountOptions: []
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: false
      name: general
      isDefault: false
      deletionPolicy: Delete
      annotations: {}
      labels: {}
      parameters: {}
    cephObjectStores:
      - name: default
        namespace: ceph
        spec:
          allowUsersInNamespaces:
            - "*"
          metadataPool:
            failureDomain: host
            replicated:
              size: 1
          dataPool:
            failureDomain: host
            replicated:
              size: 1
          preservePoolsOnDelete: true
          gateway:
            port: 8080
            instances: 1
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          name: ceph-bucket
          reclaimPolicy: Delete
          volumeBindingMode: "Immediate"
          parameters:
            region: us-east-1
